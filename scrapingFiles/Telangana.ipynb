{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\hp\\OneDrive\\Documents\\redbus_scraping\\redbusWebscraping\\utilities\n",
      "['c:\\\\Program Files\\\\Python312\\\\python312.zip', 'c:\\\\Program Files\\\\Python312\\\\DLLs', 'c:\\\\Program Files\\\\Python312\\\\Lib', 'c:\\\\Program Files\\\\Python312', '', 'C:\\\\Users\\\\hp\\\\AppData\\\\Roaming\\\\Python\\\\Python312\\\\site-packages', 'C:\\\\Users\\\\hp\\\\AppData\\\\Roaming\\\\Python\\\\Python312\\\\site-packages\\\\win32', 'C:\\\\Users\\\\hp\\\\AppData\\\\Roaming\\\\Python\\\\Python312\\\\site-packages\\\\win32\\\\lib', 'C:\\\\Users\\\\hp\\\\AppData\\\\Roaming\\\\Python\\\\Python312\\\\site-packages\\\\Pythonwin', 'c:\\\\Program Files\\\\Python312\\\\Lib\\\\site-packages']\n",
      "c:\\Users\\hp\\OneDrive\\Documents\\redbus_scraping\\redbusWebscraping\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Get the absolute path to the project folder, then to 'utilities'\n",
    "project_path = os.path.abspath(\"..\")  # This goes up one level from 'scrapingFiles'\n",
    "utilities_path = os.path.join(project_path, \"utilities\")\n",
    "print(utilities_path)\n",
    "print(sys.path)\n",
    "# Add 'utilities' folder to Python's path if it's not already there\n",
    "if utilities_path not in sys.path:\n",
    "    sys.path.append(utilities_path)\n",
    "print(project_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to scrape all pages\n",
    "\n",
    "from datetime import datetime\n",
    "from commonFunctions import exportCsv, initialize_driver, load_page, scrape_bus_routes\n",
    "from config import telanganaUrl\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "\n",
    "URL=telanganaUrl\n",
    "\n",
    "# Function to scrape bus details\n",
    "def scrape_bus_details(driver, link, name):\n",
    "    try:\n",
    "        #driver.get(url)\n",
    "        #time.sleep(5)  # Allow the page to load\n",
    "        load_page(driver,link)\n",
    "        \n",
    "        # Click the \"View Buses\" button if it exists\n",
    "        try:\n",
    "            view_buses_button = WebDriverWait(driver, 10).until(\n",
    "                EC.element_to_be_clickable((By.CLASS_NAME, \"button\"))\n",
    "            )\n",
    "            driver.execute_script(\"arguments[0].click();\", view_buses_button)\n",
    "            time.sleep(5)  # Wait for buses to load\n",
    "            \n",
    "            # Scroll down to load all bus items\n",
    "            driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "            time.sleep(5)  # Wait for the page to load more content\n",
    "            # Find bus item details\n",
    "            bus_name= driver.find_elements(By.CLASS_NAME, \"travels.lh-24.f-bold.d-color\")\n",
    "            bus_type = driver.find_elements(By.CLASS_NAME, \"bus-type.f-12.m-top-16.l-color.evBus\")\n",
    "            departing_time = driver.find_elements(By.CLASS_NAME, \"dp-time.f-19.d-color.f-bold\")\n",
    "            duration = driver.find_elements(By.CLASS_NAME, \"dur.l-color.lh-24\")\n",
    "            reaching_time = driver.find_elements(By.CLASS_NAME, \"bp-time.f-19.d-color.disp-Inline\")\n",
    "            star_rating = driver.find_elements(By.XPATH, \"//div[@class='rating-sec lh-24']\")\n",
    "            price = driver.find_elements(By.CLASS_NAME, \"fare.d-block\")\n",
    "\n",
    "            # Use XPath to handle both seat availability classes\n",
    "            seat_availability = driver.find_elements(By.XPATH, \"//div[contains(@class, 'seat-left m-top-30') or contains(@class, 'seat-left m-top-16')]\")\n",
    "\n",
    "            bus_details = []\n",
    "            for i in range(len(bus_name)): # Total number of bus found on the page\n",
    "                bus_detail = {\n",
    "                    \"Route_Name\": name,\n",
    "                    \"Route_Link\": link,\n",
    "                    \"Bus_Name\": bus_name[i].text,\n",
    "                    \"Bus_Type\": bus_type[i].text,\n",
    "                    \"Departing_Time\": departing_time[i].text,\n",
    "                    \"Duration\": duration[i].text,\n",
    "                    \"Reaching_Time\": reaching_time[i].text,\n",
    "                    \"Star_Rating\": star_rating[i].text if i < len(star_rating) else '0',\n",
    "                    \"Price\": price[i].text,\n",
    "                    \"Seat_Availability\": seat_availability[i].text if i < len(seat_availability) else '0'\n",
    "                }\n",
    "                bus_details.append(bus_detail)\n",
    "            return bus_details\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred while scraping bus details for {link}: {str(e)}\")\n",
    "            return []\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred while accessing {link}: {str(e)}\")\n",
    "        return []\n",
    "\n",
    "def scrape_all_pages():\n",
    "    for page in range(1, 4):  # There are 3 pages\n",
    "        try:\n",
    "            initializeDriver = initialize_driver()\n",
    "            load_page(initializeDriver,URL)\n",
    "            \n",
    "            if page > 1:\n",
    "                pagination_tab = WebDriverWait(initializeDriver, 10).until(\n",
    "                    EC.presence_of_element_located((By.XPATH, f\"//div[contains(@class, 'DC_117_pageTabs')][text()='{page}']\"))\n",
    "                )\n",
    "                initializeDriver.execute_script(\"arguments[0].scrollIntoView();\", pagination_tab)\n",
    "                initializeDriver.execute_script(\"arguments[0].click();\", pagination_tab)\n",
    "                time.sleep(5)  # Wait for the page to load\n",
    "            \n",
    "            all_bus_routes_link, all_bus_routes_name = scrape_bus_routes(initializeDriver)\n",
    "            # Iterate over each bus route link and scrape the details\n",
    "            for route_link, route_name in zip(all_bus_routes_link, all_bus_routes_name):\n",
    "                bus_details = scrape_bus_details(initializeDriver, route_link, route_name)\n",
    "                if bus_details:\n",
    "                    all_bus_details.extend(bus_details)\n",
    "\n",
    "            initializeDriver.quit()\n",
    "        except Exception as err:\n",
    "            print(f\"Error occurred while accessing page {page}: {str(err)}\")\n",
    "\n",
    "# List to hold all bus details\n",
    "all_bus_details = []\n",
    "\n",
    "# Scrape routes and details from all pages\n",
    "scrape_all_pages()\n",
    "\n",
    "# Generate a timestamp for the file name\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\") \n",
    "filename=f\"Telangana_bus_details_{timestamp}.csv\"\n",
    "exportCsv(all_bus_details,filename)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
